# Local LLM Agent Orchestrator (Milestone v1)

## ‚úÖ What this is
- Runs open-weight Mistral & Llama 3 locally via Ollama.
- FastAPI server with `/api/ask` endpoint.
- Handles streamed JSON properly.
- Ready to plug in LangGraph agent orchestration.

## ‚ö° How to run
```bash
# Setup
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Pull a model
ollama pull mistral

# Run FastAPI
uvicorn main:app --reload
end


# Local LLM Agent Orchestrator

## ‚úÖ What this does
- Runs Mistral/Llama 3 locally via Ollama
- FastAPI server for orchestration
- React front-end with dynamic temperature & token control

## ‚ö° How to run

### 1Ô∏è‚É£ Backend
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
ollama pull mistral
uvicorn main:app --reload


cd frontend
npm install
npm start



---

### ‚úÖ **5Ô∏è‚É£ Double-check your Git status**
In VS Code sidebar or terminal:
```bash
git status



## üöÄ Deploy locally with Docker Compose

```bash
docker-compose up --build
