# üöÄ Local LLM Agent Orchestrator

## ‚úÖ What this is

- Runs open-weight LLMs (Mistral, Llama 3) locally via Ollama.
- Provides a FastAPI server with a `/api/ask` endpoint.
- Uses LangGraph nodes for orchestration logic.
- React front-end for prompt submission, temperature, tokens, and model selection.
- Robust fallback: Handles invalid model, timeouts, or empty prompt safely.
- Docker Compose: spins up **Ollama** + **Orchestrator** together.

---

## ‚öôÔ∏è How to Run Locally (Manual Dev)

```bash
# 1Ô∏è‚É£ Clone and set up Python venv
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 2Ô∏è‚É£ Pull an LLM model
ollama pull mistral

# 3Ô∏è‚É£ Start FastAPI
uvicorn main:app --reload

# 4Ô∏è‚É£ Start React Frontend
cd frontend
npm install
npm start


# One command to build and run:
docker-compose up --build
