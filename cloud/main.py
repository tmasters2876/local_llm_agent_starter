from fastapi import FastAPI, Request
from pydantic import BaseModel
from orchestrator import run_orchestration
from fastapi.middleware.cors import CORSMiddleware
from ollama_connector import query_ollama

# âœ… Create FastAPI app
app = FastAPI()

# âœ… CORS middleware â€” configured once
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ðŸ”’ Restrict to your frontend domain in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# âœ… Health check endpoint
@app.get("/health")
def health():
    return {"status": "ok"}

# âœ… Root welcome endpoint
@app.get("/")
def root():
    return {"message": "Local LLM Agent Orchestrator is running!"}

# âœ… Input model (optional but matches expected POST schema)
class PromptRequest(BaseModel):
    prompt: str
    temperature: float = 0.7
    num_predict: int = 100
    model: str = "gemma:2b"

# âœ… Main orchestration endpoint
@app.post("/api/ask")
async def ask(request: Request):
    body = await request.json()
    result = run_orchestration(body)
    return {"result": result}

# âœ… Direct model-only endpoint for debugging
@app.post("/api/ask/test_direct")
async def test_direct(request: Request):
    body = await request.json()
    prompt = body.get("prompt", "")
    model = body.get("model", "gemma:2b")
    result = query_ollama(prompt, model=model)
    return {"result": result}
